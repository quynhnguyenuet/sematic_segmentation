{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDYIt1kcmg3m"
      },
      "source": [
        "\n",
        "\n",
        "#### Task Structure\n",
        "\n",
        "```\n",
        "1. Imports \n",
        "2. Hyperparameters\n",
        "       -- Includes learning rate, dataset path, batch size and more.\n",
        "3. Loading Dataset  \n",
        "4. Dataset and Dataloader\n",
        "       -- Initialising dataloader\n",
        "5. Model Architecture  \n",
        "       -- Defining model architecture (R2U-Net and modifications for task 3)\n",
        "6. Loss Function and Optimizers  \n",
        "       -- Adam Optimiser and Cross Entropy Loss\n",
        "7. Functions for Metrics Calculation  \n",
        "       -- Calculation of 5 metrics: Specificity, Senstivity, F1 Score, Accuracy, Jaccard Score\n",
        "8. Train Function \n",
        "9. Validation Function  \n",
        "10. Training Epochs and Validation\n",
        "       -- Perform training and calculate metrics\n",
        "11. Plotting Loss over training Epochs  \n",
        "       -- Show loss values over different parts of iteration\n",
        "12. Plot Evaluation Metrics  \n",
        "       -- Plot of 5 metrics\n",
        "13. Visualizing the results\n",
        "       -- Displays 5 predicted segmentation masks along with original segmentation masks\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGJCvGefmg3o"
      },
      "source": [
        "### 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q1G3ntsymg3p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import os\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import sklearn.metrics as skm\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt  \n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from PIL import Image\n",
        "import imageio\n",
        "from skimage.transform import resize\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeJU8zhtmg3q"
      },
      "source": [
        "### 2. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gJy0Yqszmg3q"
      },
      "outputs": [],
      "source": [
        "# replace device accordingly\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "# replace with location of folder containing \"gtFine\" and \"leftImg8bit\"\n",
        "path_data = \"/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/data\"\n",
        "\n",
        "learning_rate = 1e-6\n",
        "train_epochs = 8\n",
        "n_classes = 19\n",
        "batch_size = 4\n",
        "num_workers = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcTqmMh_mg3q"
      },
      "source": [
        "### 3. Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Be4eJP8Nmg3r"
      },
      "outputs": [],
      "source": [
        "# Adapted from dataset loader written by meetshah1995 with modifications\n",
        "# https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/loader/cityscapes_loader.py\n",
        "\n",
        "def recursive_glob(rootdir=\".\", suffix=\"\"):\n",
        "    return [\n",
        "        os.path.join(looproot, filename)\n",
        "        for looproot, _, filenames in os.walk(rootdir)\n",
        "        for filename in filenames\n",
        "        if filename.endswith(suffix)\n",
        "    ]\n",
        "\n",
        "\n",
        "class cityscapesLoader(data.Dataset):\n",
        "    colors = [  # [  0,   0,   0],\n",
        "        [128, 64, 128],\n",
        "        [244, 35, 232],\n",
        "        [70, 70, 70],\n",
        "        [102, 102, 156],\n",
        "        [190, 153, 153],\n",
        "        [153, 153, 153],\n",
        "        [250, 170, 30],\n",
        "        [220, 220, 0],\n",
        "        [107, 142, 35],\n",
        "        [152, 251, 152],\n",
        "        [0, 130, 180],\n",
        "        [220, 20, 60],\n",
        "        [255, 0, 0],\n",
        "        [0, 0, 142],\n",
        "        [0, 0, 70],\n",
        "        [0, 60, 100],\n",
        "        [0, 80, 100],\n",
        "        [0, 0, 230],\n",
        "        [119, 11, 32],\n",
        "    ]\n",
        "\n",
        "    # makes a dictionary with key:value. For example 0:[128, 64, 128]\n",
        "    label_colours = dict(zip(range(19), colors))\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        # which data split to use\n",
        "        split=\"train\",\n",
        "        # transform function activation\n",
        "        is_transform=True,\n",
        "        # image_size to use in transform function\n",
        "        img_size=(512, 1024),\n",
        "    ):\n",
        "        self.root = root\n",
        "        self.split = split\n",
        "        self.is_transform = is_transform\n",
        "        self.n_classes = 19\n",
        "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
        "        self.files = {}\n",
        "\n",
        "        # makes it: /raid11/cityscapes/ + leftImg8bit + train (as we named the split folder this)\n",
        "        self.images_base = os.path.join(self.root, \"leftImg8bit\", self.split)\n",
        "        self.annotations_base = os.path.join(self.root, \"gtFine\", self.split)\n",
        "        \n",
        "        # contains list of all pngs inside all different folders. Recursively iterates \n",
        "        self.files[split] = recursive_glob(rootdir=self.images_base, suffix=\".png\")\n",
        "\n",
        "        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n",
        "        \n",
        "        # these are 19\n",
        "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33,\n",
        "        ]\n",
        "        \n",
        "        # these are 19 + 1; \"unlabelled\" is extra\n",
        "        self.class_names = [\n",
        "            \"unlabelled\",\n",
        "            \"road\",\n",
        "            \"sidewalk\",\n",
        "            \"building\",\n",
        "            \"wall\",\n",
        "            \"fence\",\n",
        "            \"pole\",\n",
        "            \"traffic_light\",\n",
        "            \"traffic_sign\",\n",
        "            \"vegetation\",\n",
        "            \"terrain\",\n",
        "            \"sky\",\n",
        "            \"person\",\n",
        "            \"rider\",\n",
        "            \"car\",\n",
        "            \"truck\",\n",
        "            \"bus\",\n",
        "            \"train\",\n",
        "            \"motorcycle\",\n",
        "            \"bicycle\",\n",
        "        ]\n",
        "        \n",
        "        # for void_classes; useful for loss function\n",
        "        self.ignore_index = 250\n",
        "        \n",
        "        # dictionary of valid classes 7:0, 8:1, 11:2\n",
        "        self.class_map = dict(zip(self.valid_classes, range(19)))\n",
        "\n",
        "        if not self.files[split]:\n",
        "            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images_base))\n",
        "        \n",
        "        # prints number of images found\n",
        "        print(\"Found %d %s images\" % (len(self.files[split]), split))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # path of image\n",
        "        img_path = self.files[self.split][index].rstrip()\n",
        "        \n",
        "        # path of label\n",
        "        lbl_path = os.path.join(\n",
        "            self.annotations_base,\n",
        "            img_path.split(os.sep)[-2],\n",
        "            os.path.basename(img_path)[:-15] + \"gtFine_labelIds.png\",\n",
        "        )\n",
        "\n",
        "        # read image\n",
        "        img = imageio.imread(img_path)\n",
        "        # convert to numpy array\n",
        "        img = np.array(img, dtype=np.uint8)\n",
        "\n",
        "        # read label\n",
        "        lbl = imageio.imread(lbl_path)\n",
        "        # encode using encode_segmap function: 0...18 and 250\n",
        "        lbl = self.encode_segmap(np.array(lbl, dtype=np.uint8))\n",
        "\n",
        "        if self.is_transform:\n",
        "            img, lbl = self.transform(img, lbl)\n",
        "        \n",
        "        return img, lbl\n",
        "\n",
        "    def transform(self, img, lbl):       \n",
        "        # Image resize; I think imresize outputs in different format than what it received\n",
        "        img = cv2.resize(img, (self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\n",
        "        # change to BGR\n",
        "        img = img[:, :, ::-1]  # RGB -> BGR\n",
        "        # change data type to float64\n",
        "        img = img.astype(np.float64)\n",
        "        # subtract mean\n",
        "        # NHWC -> NCHW\n",
        "        img = img.transpose(2, 0, 1)\n",
        "\n",
        "        \n",
        "        classes = np.unique(lbl)\n",
        "        lbl = lbl.astype(float)\n",
        "        lbl = cv2.resize(lbl, (self.img_size[0], self.img_size[1]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        lbl = lbl.astype(int)\n",
        "\n",
        "        if not np.all(classes == np.unique(lbl)):\n",
        "            print(\"WARN: resizing labels yielded fewer classes\")\n",
        "\n",
        "        if not np.all(np.unique(lbl[lbl != self.ignore_index]) < self.n_classes):\n",
        "            print(\"after det\", classes, np.unique(lbl))\n",
        "            raise ValueError(\"Segmentation map contained invalid class values\")\n",
        "\n",
        "        img = torch.from_numpy(img).float()\n",
        "        lbl = torch.from_numpy(lbl).long()\n",
        "\n",
        "        return img, lbl\n",
        "      \n",
        "    def decode_segmap(self, temp):\n",
        "        r = temp.copy()\n",
        "        g = temp.copy()\n",
        "        b = temp.copy()\n",
        "        for l in range(0, self.n_classes):\n",
        "            r[temp == l] = self.label_colours[l][0]\n",
        "            g[temp == l] = self.label_colours[l][1]\n",
        "            b[temp == l] = self.label_colours[l][2]\n",
        "\n",
        "        rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
        "        rgb[:, :, 0] = r / 255.0\n",
        "        rgb[:, :, 1] = g / 255.0\n",
        "        rgb[:, :, 2] = b / 255.0\n",
        "        return rgb\n",
        "\n",
        "    # there are different class 0...33\n",
        "    # we are converting that info to 0....18; and 250 for void classes\n",
        "    # final mask has values 0...18 and 250\n",
        "    def encode_segmap(self, mask):\n",
        "        # !! Comment in code had wrong informtion\n",
        "        # Put all void classes to ignore_index\n",
        "        for _voidc in self.void_classes:\n",
        "            mask[mask == _voidc] = self.ignore_index\n",
        "        for _validc in self.valid_classes:\n",
        "            mask[mask == _validc] = self.class_map[_validc]\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfosXqIcmg3t"
      },
      "source": [
        "### 4. Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Df3FGi--mg3u",
        "outputId": "b7f6aba2-106b-4d57-8283-dabe61b6f806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2975 train images\n",
            "Found 500 val images\n"
          ]
        }
      ],
      "source": [
        "train_data = cityscapesLoader(\n",
        "    root = path_data, \n",
        "    split='train'\n",
        "    )\n",
        "\n",
        "val_data = cityscapesLoader(\n",
        "    root = path_data, \n",
        "    split='val'\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size = batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers = num_workers,\n",
        "    #pin_memory = pin_memory  # gave no significant advantage\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_data,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = num_workers,\n",
        "    #pin_memory = pin_memory  # gave no significant advantage\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWhmDKFgmg3v"
      },
      "source": [
        "### 5. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IjR1J0zMmg3w"
      },
      "outputs": [],
      "source": [
        "class Up_Sample_Conv(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(Up_Sample_Conv, self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), # Nearest neighbour for upsampling are two \n",
        "            nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class Repeat(nn.Module):\n",
        "    def __init__(self, ch_out):\n",
        "        super(Repeat, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)) \n",
        "#Inplace has been set to TRUE so that it modifies the input directly, without allocating any additional output.\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(2):\n",
        "            if i == 0:\n",
        "                x_rec = self.conv(x)\n",
        "            x_rec = self.conv(x + x_rec)\n",
        "        return x_rec\n",
        "\n",
        "class RR_Conv(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(RR_Conv, self).__init__()\n",
        "        self.Repeat_block = nn.Sequential(Repeat(ch_out),Repeat(ch_out))\n",
        "        self.Conv = nn.Conv2d(ch_in, ch_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, input_img):\n",
        "        input_img = self.Conv(input_img)\n",
        "        conv_input_img = self.Repeat_block(input_img)\n",
        "        return input_img + conv_input_img \n",
        "    \n",
        "############\n",
        "############\n",
        "\n",
        "class R2U_Net(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=19):\n",
        "        super(R2U_Net, self).__init__()\n",
        "        \n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.channel_1 = 64 # R2U-net activation maps in first layer\n",
        "        self.channel_2 = 2*self.channel_1\n",
        "        self.channel_3 = 2*self.channel_2\n",
        "        self.channel_4 = 2*self.channel_3\n",
        "        self.channel_5 = 2*self.channel_4\n",
        "        \n",
        "        # For new layer added\n",
        "        self.channel_6 = 2*self.channel_5\n",
        "        \n",
        "        self.channels = [self.channel_1, self.channel_2, self.channel_3, self.channel_4, self.channel_5, self.channel_6]\n",
        "            \n",
        "        '''Performs Convolution and responsible for the encoding part of the architecture'''    \n",
        "        self.Layer1 = RR_Conv(img_ch, self.channels[0])\n",
        "        self.Layer2 = RR_Conv(self.channels[0], self.channels[1])\n",
        "        self.Layer3 = RR_Conv(self.channels[1], self.channels[2])\n",
        "        self.Layer4 = RR_Conv(self.channels[2], self.channels[3])\n",
        "        self.Layer5 = RR_Conv(self.channels[3], self.channels[4])\n",
        "        '''Addition of convolutional layer (Depth increased)'''\n",
        "        self.Layer6 = RR_Conv(self.channels[4], self.channels[5]) # For extra depth\n",
        "\n",
        "        '''Below function calls are responsible for the decoding part of the architeture'''\n",
        "        \n",
        "        '''Upsamples the input and then performs convolution followed by ReLU'''\n",
        "        self.DeConvLayer6 = Up_Sample_Conv(self.channels[5], self.channels[4]) # For extra depth\n",
        "        self.DeConvLayer5 = Up_Sample_Conv(self.channels[4], self.channels[3])\n",
        "        self.DeConvLayer4 = Up_Sample_Conv(self.channels[3],self.channels[2])\n",
        "        self.DeConvLayer3 = Up_Sample_Conv(self.channels[2], self.channels[1])\n",
        "        self.DeConvLayer2 = Up_Sample_Conv(self.channels[1], self.channels[0])\n",
        "        \n",
        "        '''Responsible for computation in Recurrent Residual Blocks'''\n",
        "        self.Up_Layer6 = RR_Conv(self.channels[5], self.channels[4]) # For extra depth\n",
        "        self.Up_Layer5 = RR_Conv(self.channels[4], self.channels[3])\n",
        "        self.Up_Layer4 = RR_Conv(self.channels[3], self.channels[2])\n",
        "        self.Up_Layer3 = RR_Conv(self.channels[2], self.channels[1])\n",
        "        self.Up_Layer2 = RR_Conv(self.channels[1], self.channels[0])\n",
        "        \n",
        "        '''Final output of the architecture needs to have output channels=number of class labels(19)'''\n",
        "        self.Conv = nn.Conv2d(self.channels[0], output_ch, kernel_size=1, stride=1, padding=0)        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        '''Recurrent Convolution'''\n",
        "        conv1 = self.Layer1(x)\n",
        "        mp1 = self.MaxPool(conv1)\n",
        "        conv2 = self.Layer2(mp1)\n",
        "        mp2 = self.MaxPool(conv2)\n",
        "        conv3 = self.Layer3(mp2)\n",
        "        mp3 = self.MaxPool(conv3)\n",
        "        conv4 = self.Layer4(mp3)\n",
        "        mp4 = self.MaxPool(conv4)\n",
        "        conv5 = self.Layer5(mp4)\n",
        "        \n",
        "        '''For one extra depth'''\n",
        "        mp5 = self.MaxPool(conv5)\n",
        "        conv6 = self.Layer6(mp5)\n",
        "        '''--------------------'''\n",
        "\n",
        "        ''' \n",
        "        Decoder part of the architecture which performs \n",
        "        Recurrent up convolution as well as concatention from previous layers \n",
        "        '''\n",
        "        \n",
        "        '''For one extra depth'''\n",
        "        deconv6 = self.DeConvLayer6(conv6)\n",
        "        deconv6 = torch.cat((conv5, deconv6), dim=1)\n",
        "        deconv6 = self.Up_Layer6(deconv6)\n",
        "        '''--------------------'''\n",
        "        \n",
        "        deconv5 = self.DeConvLayer5(deconv6)\n",
        "        deconv5 = torch.cat((conv4, deconv5), dim=1)\n",
        "        deconv5 = self.Up_Layer5(deconv5)\n",
        "        deconv4 = self.DeConvLayer4(deconv5)\n",
        "        deconv4 = torch.cat((conv3, deconv4), dim=1)\n",
        "        deconv4 = self.Up_Layer4(deconv4)\n",
        "        deconv3 = self.DeConvLayer3(deconv4)\n",
        "        deconv3 = torch.cat((conv2, deconv3), dim=1)\n",
        "        deconv3 = self.Up_Layer3(deconv3)\n",
        "        deconv2 = self.DeConvLayer2(deconv3)\n",
        "        deconv2 = torch.cat((conv1, deconv2), dim=1)\n",
        "        deconv2 = self.Up_Layer2(deconv2)\n",
        "        deconv1 = self.Conv(deconv2)\n",
        "\n",
        "        return deconv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2GL-3Fw3mg3y"
      },
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 3.94 GiB total capacity; 2.69 GiB already allocated; 54.00 MiB free; 2.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Instance of the model defined above.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m R2U_Net()\u001b[39m.\u001b[39;49mto(device)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping similar frames: Module._apply at line 639 (2 times)]\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:985\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    983\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    984\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 985\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 3.94 GiB total capacity; 2.69 GiB already allocated; 54.00 MiB free; 2.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# Instance of the model defined above.\n",
        "model = R2U_Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS_5_nkfmg3z"
      },
      "source": [
        "### 6. Loss Function and Optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wi2Ppa9mg30"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Cross Entropy Loss adapted from meetshah1995 to prevent size inconsistencies between model precition \n",
        "# and target label\n",
        "# https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/loss/loss.py\n",
        "\n",
        "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
        "    n, c, h, w = input.size()\n",
        "    nt, ht, wt = target.size()\n",
        "\n",
        "    # Handle inconsistent size between input and target\n",
        "    if h != ht and w != wt:  # upsample labels\n",
        "        input = F.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
        "    target = target.view(-1)\n",
        "    loss = F.cross_entropy(\n",
        "        input, target, weight=weight, size_average=size_average, ignore_index=250\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvC3SLnjmg30"
      },
      "source": [
        "### 7. Functions for Metrics Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVmCM6wnmg30"
      },
      "outputs": [],
      "source": [
        "'''We have used skelarn libraries to calculate Accuracy and Jaccard Score'''\n",
        "\n",
        "def get_metrics(gt_label, pred_label):\n",
        "    #Accuracy Score\n",
        "    acc = skm.accuracy_score(gt_label, pred_label, normalize=True)\n",
        "    \n",
        "    #Jaccard Score/IoU\n",
        "    js = skm.jaccard_score(gt_label, pred_label, average='micro')\n",
        "    \n",
        "    result_gm_sh = [acc, js]\n",
        "    return(result_gm_sh)\n",
        "\n",
        "'''\n",
        "Calculation of confusion matrix from :\n",
        "https://github.com/meetshah1995/pytorch-semseg/blob/master/ptsemseg/metrics.py\n",
        "\n",
        "Added modifications to calculate 3 evaluation metrics - \n",
        "Specificity, Senstivity, F1 Score\n",
        "'''\n",
        "\n",
        "class runningScore(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "    def _fast_hist(self, label_true, label_pred, n_class):\n",
        "        mask = (label_true >= 0) & (label_true < n_class)\n",
        "        hist = np.bincount(\n",
        "            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n",
        "        ).reshape(n_class, n_class)\n",
        "        return hist\n",
        "\n",
        "    def update(self, label_trues, label_preds):\n",
        "        for lt, lp in zip(label_trues, label_preds):\n",
        "            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n",
        "\n",
        "    def get_scores(self):\n",
        "        # confusion matrix\n",
        "        hist = self.confusion_matrix\n",
        "        \n",
        "        #              T\n",
        "        #         0    1    2\n",
        "        #    0   TP   FP   FP\n",
        "        #  P 1   FN   TN   TN       This is wrt to class 0\n",
        "        #    2   FN   TN   TN\n",
        "\n",
        "        #         0    1    2\n",
        "        #    0   TP   FP   FP\n",
        "        #  P 1   FP   TP   FP       This is wrt prediction classes; AXIS = 1\n",
        "        #    2   FP   FP   TP \n",
        "\n",
        "        #         0    1    2\n",
        "        #    0   TP   FN   FN\n",
        "        #  P 1   FN   TP   FN       This is wrt true classes; AXIS = 0\n",
        "        #    2   FN   FN   TP   \n",
        "\n",
        "        TP = np.diag(hist)\n",
        "        TN = hist.sum() - hist.sum(axis = 1) - hist.sum(axis = 0) + np.diag(hist)\n",
        "        FP = hist.sum(axis = 1) - TP\n",
        "        FN = hist.sum(axis = 0) - TP\n",
        "        \n",
        "        # 1e-6 was added to prevent corner cases where denominator = 0\n",
        "        \n",
        "        # Specificity: TN / TN + FP\n",
        "        specif_cls = (TN) / (TN + FP + 1e-6)\n",
        "        specif = np.nanmean(specif_cls)\n",
        "        \n",
        "        # Senstivity/Recall: TP / TP + FN\n",
        "        sensti_cls = (TP) / (TP + FN + 1e-6)\n",
        "        sensti = np.nanmean(sensti_cls)\n",
        "        \n",
        "        # Precision: TP / (TP + FP)\n",
        "        prec_cls = (TP) / (TP + FP + 1e-6)\n",
        "        prec = np.nanmean(prec_cls)\n",
        "        \n",
        "        # F1 = 2 * Precision * Recall / Precision + Recall\n",
        "        f1 = (2 * prec * sensti) / (prec + sensti + 1e-6)\n",
        "        \n",
        "        return (\n",
        "            {\n",
        "                \"Specificity\": specif,\n",
        "                \"Senstivity\": sensti,\n",
        "                \"F1\": f1,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Vko8IWmg31"
      },
      "source": [
        "### 8. Train Function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mBf3dJhmg31"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, optimizer, epoch_i, epoch_total):\n",
        "        count = 0\n",
        "        \n",
        "        # List to cumulate loss during iterations\n",
        "        loss_list = []\n",
        "        for (images, labels) in train_loader:\n",
        "            count += 1\n",
        "            \n",
        "            # we used model.eval() below. This is to bring model back to training mood.\n",
        "            model.train()\n",
        "\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Model Prediction\n",
        "            pred = model(images)\n",
        "            \n",
        "            # Loss Calculation\n",
        "            loss = cross_entropy2d(pred, labels)\n",
        "            loss_list.append(loss)\n",
        "\n",
        "            # optimiser\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # interval to print train statistics\n",
        "            if count % 50 == 0:\n",
        "                fmt_str = \"Image: {:d} in epoch: [{:d}/{:d}]  and Loss: {:.4f}\"\n",
        "                print_str = fmt_str.format(\n",
        "                    count,\n",
        "                    epoch_i + 1,\n",
        "                    epoch_total,\n",
        "                    loss.item()\n",
        "                )\n",
        "                print(print_str)\n",
        "                   \n",
        "#           # break for testing purpose\n",
        "#             if count == 10:\n",
        "#                 break\n",
        "        return(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpHXrraimg31"
      },
      "source": [
        "### 9. Validation Function  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpMGYGXcmg32"
      },
      "outputs": [],
      "source": [
        "def validate(val_loader, model, epoch_i):\n",
        "    \n",
        "    # tldr: to make layers behave differently during inference (vs training)\n",
        "    model.eval()\n",
        "    \n",
        "    # enable calculation of confusion matrix for n_classes = 19\n",
        "    running_metrics_val = runningScore(19)\n",
        "    \n",
        "    # empty list to add Accuracy and Jaccard Score Calculations\n",
        "    acc_sh = []\n",
        "    js_sh = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for image_num, (val_images, val_labels) in tqdm(enumerate(val_loader)):\n",
        "            \n",
        "            val_images = val_images.to(device)\n",
        "            val_labels = val_labels.to(device)\n",
        "            \n",
        "            # Model prediction\n",
        "            val_pred = model(val_images)\n",
        "            \n",
        "            # Coverting val_pred from (1, 19, 512, 1024) to (1, 512, 1024)\n",
        "            # considering predictions with highest scores for each pixel among 19 classes\n",
        "            pred = val_pred.data.max(1)[1].cpu().numpy()\n",
        "            gt = val_labels.data.cpu().numpy()\n",
        "            \n",
        "            # Updating Mertics\n",
        "            running_metrics_val.update(gt, pred)\n",
        "            sh_metrics = get_metrics(gt.flatten(), pred.flatten())\n",
        "            acc_sh.append(sh_metrics[0])\n",
        "            js_sh.append(sh_metrics[1])\n",
        "                               \n",
        "#            # break for testing purpose\n",
        "#             if image_num == 10:\n",
        "#                 break                \n",
        "\n",
        "    score = running_metrics_val.get_scores()\n",
        "    running_metrics_val.reset()\n",
        "    \n",
        "    acc_s = sum(acc_sh)/len(acc_sh)\n",
        "    js_s = sum(js_sh)/len(js_sh)\n",
        "    score[\"acc\"] = acc_s\n",
        "    score[\"js\"] = js_s\n",
        "    \n",
        "    print(\"Different Metrics were: \", score)  \n",
        "    return(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90-dhONlmg32"
      },
      "source": [
        "### 10. Training Epochs and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhQaQQJLmg32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_64771/1976187924.py:120: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  img = imageio.imread(img_path)\n",
            "/tmp/ipykernel_64771/1976187924.py:125: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  lbl = imageio.imread(lbl_path)\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 3.94 GiB total capacity; 2.59 GiB already allocated; 88.88 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch_i\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss_i \u001b[39m=\u001b[39m train(train_loader, model, optimizer, epoch_i, train_epochs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss_all_epochs\u001b[39m.\u001b[39mappend(loss_i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m t2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "\u001b[1;32m/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Model Prediction\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pred \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Loss Calculation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy2d(pred, labels)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Recurrent Convolution'''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     conv1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayer1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     mp1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMaxPool(conv1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     conv2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayer2(mp1)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_img):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     input_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConv(input_img)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     conv_input_img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mRepeat_block(input_img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m input_img \u001b[39m+\u001b[39m conv_input_img\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         x_rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     x_rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x \u001b[39m+\u001b[39;49m x_rec)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/quynhnguyen/Downloads/Cityscapes-Segmentation-master/Cityscapes_modified_R2UNET.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x_rec\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 3.94 GiB total capacity; 2.59 GiB already allocated; 88.88 MiB free; 2.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # to hold loss values after each epoch\n",
        "    loss_all_epochs = []\n",
        "    \n",
        "    # to hold different metrics after each epoch\n",
        "    Specificity_ = []\n",
        "    Senstivity_ = []\n",
        "    F1_ = []\n",
        "    acc_ = []\n",
        "    js_ = []\n",
        "    \n",
        "    for epoch_i in range(train_epochs):\n",
        "        # training\n",
        "        print(f\"Epoch {epoch_i + 1}\\n-------------------------------\")\n",
        "        t1 = time.time()\n",
        "        loss_i = train(train_loader, model, optimizer, epoch_i, train_epochs)\n",
        "        loss_all_epochs.append(loss_i)\n",
        "        t2 = time.time()\n",
        "        print(\"It took: \", t2-t1, \" unit time\")\n",
        "\n",
        "        # metrics calculation on validation data\n",
        "        dummy_list = validate(val_loader, model, epoch_i)   \n",
        "        \n",
        "        # Add metrics to empty list above\n",
        "        Specificity_.append(dummy_list[\"Specificity\"])\n",
        "        Senstivity_.append(dummy_list[\"Senstivity\"])\n",
        "        F1_.append(dummy_list[\"F1\"])\n",
        "        acc_.append(dummy_list[\"acc\"])\n",
        "        js_.append(dummy_list[\"js\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9t_MAShmg32"
      },
      "source": [
        "### 11. Plotting Loss over training Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns2HGe6Xmg33"
      },
      "outputs": [],
      "source": [
        "# loss_all_epochs: contains 2d list of tensors with: (epoch, loss tensor)\n",
        "# converting to 1d list for plotting\n",
        "loss_1d_list = [item for sublist in loss_all_epochs for item in sublist]\n",
        "loss_list_numpy = []\n",
        "for i in range(len(loss_1d_list)):\n",
        "    z = loss_1d_list[i].cpu().detach().numpy()\n",
        "    loss_list_numpy.append(z)\n",
        "plt.xlabel(\"Images used in training epochs\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.plot(loss_list_numpy)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXFG63lZmg33"
      },
      "source": [
        "### 12. Plot Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fze7SO0mg33"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "\n",
        "x = [i for i in range(1, train_epochs + 1)]\n",
        "\n",
        "# plot 5 metrics: Specificity, Senstivity, F1 Score, Accuracy, Jaccard Score\n",
        "plt.plot(x,Specificity_, label='Specificity')\n",
        "plt.plot(x,Senstivity_, label='Senstivity')\n",
        "plt.plot(x,F1_, label='F1 Score')\n",
        "plt.plot(x,acc_, label='Accuracy')\n",
        "plt.plot(x,js_, label='Jaccard Score')\n",
        "\n",
        "plt.grid(linestyle = '--', linewidth = 0.5)\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVfSUC_vmg33"
      },
      "source": [
        "### 13. Visualizing the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR4Ca3H_mg33"
      },
      "outputs": [],
      "source": [
        "# tldr: to make layers behave differently during inference (vs training)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for image_num, (val_images, val_labels) in tqdm(enumerate(val_loader)):\n",
        "\n",
        "        val_images = val_images.to(device)\n",
        "        val_labels = val_labels.to(device)\n",
        "        \n",
        "        # model prediction\n",
        "        val_pred = model(val_images)\n",
        "\n",
        "        # Coverting val_pred from (1, 19, 512, 1024) to (1, 512, 1024)\n",
        "        # considering predictions with highest scores for each pixel among 19 classes        \n",
        "        prediction = val_pred.data.max(1)[1].cpu().numpy()\n",
        "        ground_truth = labels_val.data.cpu().numpy()\n",
        "\n",
        "        # replace 100 to change number of images to print. \n",
        "        # 500 % 100 = 5. So, we will get 5 predictions and ground truths\n",
        "        if image_num % 100 == 0:\n",
        "            \n",
        "            # Model Prediction\n",
        "            decoded_pred = val_data.decode_segmap(prediction[0])\n",
        "            plt.imshow(decoded_pred)\n",
        "            plt.show()\n",
        "            plt.clf()\n",
        "            \n",
        "            # Ground Truth\n",
        "            decode_gt = val_data.decode_segmap(ground_truth[0])\n",
        "            plt.imshow(decode_gt)\n",
        "            plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Cityscapes_modified_R2UNET.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
